import groovy.json.JsonSlurper

apply plugin: 'wrapper'

if(JavaVersion.current() != JavaVersion.VERSION_1_7){
    throw new GradleException("==== This build must be run with java 7 ====")
}

buildscript {
  repositories {
    maven { url "https://plugins.gradle.org/m2" }
    jcenter()
  }
  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.10"
    classpath "org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.10:0.7.2"
    classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.2'
  }
}

allprojects {
  // We want to see all test results.  This is equivalatent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    //maven { url "http://dl.bintray.com/spark-jobserver/maven" }
    jcenter()
    maven { url "https://repository.apache.org/content/repositories/releases" }
    maven { url "https://repository.jboss.org/nexus/content/repositories/releases" }
    maven { url "https://repo.eclipse.org/content/repositories/paho-releases" }
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos" }
    maven { url "https://oss.sonatype.org/content/repositories/orgspark-project-1113" }
    maven { url "http://repository.mapr.com/maven" }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "http://maven.twttr.com" }
    maven { url "http://repository.apache.org/snapshots" }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'idea'
  apply plugin: 'eclipse'

  group = 'io.snappydata'
  version = '0.1.0-SNAPSHOT'

  // apply compiler options
  sourceCompatibility = 1.7
  targetCompatibility = 1.7

  compileJava.options.encoding = 'UTF-8'
  compileJava.options.compilerArgs << '-Xlint:all,-serial,-path'
  javadoc.options.charSet = 'UTF-8'

  ext {
    scalaBinaryVersion = '2.10'
    scalaVersion = scalaBinaryVersion + '.6'
    sparkVersion = '1.6.0-SNAPSHOT'
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.12'
    junitVersion = '4.11'
    hadoopVersion = '2.4.1'
    gemfireXDVersion = '2.0-Beta'
    buildFlags = ''
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }

  ext {
    testResultsBase = "${rootProject.buildDir}/tests/snappy"
    snappyProductDir = "${rootProject.buildDir}/snappy"
  }
}

def getProcessId() {
  def name = java.lang.management.ManagementFactory.getRuntimeMXBean().getName()
  return name[0..name.indexOf("@")-1]
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

// Configure scalaStyle for only non spark related modules
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/)}) {
  scalaStyle {
    configLocation = "scalastyle-config.xml"
    source = "src/main/scala"
  }
}

def cleanIntermediateFiles(def projectName) {
  def projDir = "${project(projectName).projectDir}"
  delete "${projDir}/metastore_db"
  delete "${projDir}/warehouse"
  delete "${projDir}/datadictionary"
  delete fileTree(projDir) {
    include 'BACKUPGFXD-DEFAULT-DISKSTORE**', 'locator*.dat'
  }
}
task cleanScalaTest << {
  def workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanJUnit << {
  def workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
}
task cleanDUnit << {
  def workingDir = "${testResultsBase}/dunit"
  delete workingDir
  file(workingDir).mkdirs()
}

subprojects {
  // the run task for a selected sub-project
  task run(type:JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(",") as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs '-Xmx2g', '-XX:MaxPermSize=512m'
  }

  task scalaTest(type: Test) {
    actions = [ new com.github.maiflai.ScalaTestAction() ]
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    maxHeapSize '8g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC', '-XX:+CMSPermGenSweepingEnabled',  '-XX:MaxPermSize=4G', '-ea'
    testLogging.exceptionFormat = 'full'

    List<String> suites = []
    extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
    extensions.add("suite", { String name -> suites.add(name) } )
    extensions.add("suites", { String... name -> suites.addAll(name) } )

    // running a single scala suite
    if (rootProject.hasProperty('singleSuite')) {
      suite singleSuite
    }
    workingDir = "${testResultsBase}/scalatest"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    maxParallelForks = (2 * Runtime.getRuntime().availableProcessors())
    maxHeapSize '1g'
    jvmArgs '-XX:+HeapDumpOnOutOfMemoryError', '-XX:MaxPermSize=350M', '-ea'
    testLogging.exceptionFormat = 'full'

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }

  gradle.taskGraph.whenReady({ graph ->
    tasks.withType(Test).each { test ->
      test.configure {
        onlyIf { !Boolean.getBoolean('skip.tests') }
        environment 'SNAPPY_HOME': snappyProductDir,
          'SNAPPY_DIST_CLASSPATH': "${sourceSets.test.runtimeClasspath.asPath}"

        def eol = System.getProperty('line.separator')
        beforeTest { desc ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Starting test ${desc.className} ${desc.name}${eol}"
          output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
        }
        onOutput { desc, event ->
          def output = new File(workingDir, "output.txt")
          output  << event.message
        }
        afterTest { desc, result ->
          def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
          def progress = new File(workingDir, "progress.txt")
          def output = new File(workingDir, "output.txt")
          progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
          output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
          result.exceptions.each { t ->
            progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
            output << "${getStackTrace(t)}${eol}"
          }
        }
      }
    }
  })
  check.dependsOn test, scalaTest

  // apply default manifest
  jar {
    manifest {
      attributes(
        "Manifest-Version"  : "1.0",
        "Created-By"        : System.getProperty("user.name"),
        "Title"             : rootProject.name,
        "Version"           : version,
        "Vendor"            : "Snappy Data, Inc."
      )
    }
  }

  task packageSources(type: Jar, dependsOn: classes) {
    classifier = 'sources'
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    classifier = 'sources'
    from javadoc.destinationDir
  }
  /*
  artifacts {
    archives packageSources
    archives packageDocs
  }
  */

  configurations {
    provided {
      description 'a dependency that is provided externally at runtime'
      visible true
    }

    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
    /*
    all {
      resolutionStrategy {
        // fail eagerly on version conflict (includes transitive dependencies)
        // e.g. multiple different versions of the same dependency (group and name are equal)
        failOnVersionConflict()
      }
    }
    */
  }

  task packageTests(type: Jar) {
    from sourceSets.test.output
    classifier = 'tests'
  }
  artifacts {
    testOutput packageTests
  }

  idea {
    module {
      scopes.PROVIDED.plus += [ configurations.provided ]
    }
  }

  sourceSets {
    main.compileClasspath += configurations.provided
    main.runtimeClasspath -= configurations.provided
    test.compileClasspath += configurations.provided
    test.runtimeClasspath += configurations.provided
  }

  javadoc.classpath += configurations.provided

  dependencies {
    compile 'log4j:log4j:' + log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
  }
}

task generateSources {
  dependsOn ':snappy-spark:snappy-spark-streaming-flume-sink_' + scalaBinaryVersion + ':generateAvroJava'
  dependsOn ':snappy-store:generateSources'
}

task product {
  dependsOn ":snappy-tools_${scalaBinaryVersion}:shadowJar"
  dependsOn ':snappy-aqp:jar'
  dependsOn ':snappy-examples:jar'

  doFirst {
    delete snappyProductDir
    file("${snappyProductDir}/lib").mkdirs()
  }
  doLast {
    // copy datanucleus jars specifically since they don't work as part of fat jar
    def datanucleusJars = project(":snappy-spark:snappy-spark-hive_${scalaBinaryVersion}").configurations.runtime.filter {
      it.getName().contains('datanucleus')
    }
    copy {
      from datanucleusJars
      into "${snappyProductDir}/lib"
    }
    // copy GemFireXD shared libraries for optimized JNI calls
    copy {
      from "${project(':snappy-store:gemfirexd:core').projectDir}/lib"
      into "${snappyProductDir}/lib"
    }

    // create the RELEASE file
    def release = file("${snappyProductDir}/RELEASE")
    def gitCommitId = "git rev-parse HEAD".execute().text.trim()
    release << "Snappy Spark ${project.version} ${gitCommitId} built for Hadoop $hadoopVersion\n"
    release << "Build flags: ${buildFlags}\n"

    def toolsProject = project(":snappy-tools_${scalaBinaryVersion}")
    def aqpProject = project(':snappy-aqp')
    def examplesProject = project(':snappy-examples')
    def baseName = 'snappy-spark-assembly'
    def archiveName = "${baseName}_${scalaBinaryVersion}-${version}-hadoop${hadoopVersion}.jar"
    def exampleArchiveName = "quickstart-${version}.jar"
    copy {
      from("${toolsProject.buildDir}/libs")
      into "${snappyProductDir}/lib"
      include "${toolsProject.shadowJar.archiveName}"
      rename { filename -> archiveName }
    }
    copy {
      from("${aqpProject.buildDir}/libs")
      into "${snappyProductDir}/lib"
      include "${aqpProject.jar.archiveName}"
    }
    copy {
      from "${examplesProject.buildDir}/libs"
      into "${snappyProductDir}/lib"
      include "${examplesProject.jar.archiveName}"
      rename { filename -> exampleArchiveName }
    }
    copy {
      from "${project(':snappy-spark').projectDir}/bin"
      into "${snappyProductDir}/bin"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/sbin"
      into "${snappyProductDir}/sbin"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/conf"
      into "${snappyProductDir}/conf"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/python"
      into "${snappyProductDir}/python"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/data"
      into "${snappyProductDir}/data"
    }
    copy {
      from("${toolsProject.projectDir}/bin")
      into "${snappyProductDir}/bin"
    }
    copy {
      from("${toolsProject.projectDir}/sbin")
      into "${snappyProductDir}/sbin"
    }
    copy {
      from("${toolsProject.projectDir}/conf")
      into "${snappyProductDir}/conf"
    }
    copy {
      from("${examplesProject.projectDir}/quickstart")
      into "${snappyProductDir}/quickstart"
    }

    def sparkR = "${project(':snappy-spark').projectDir}/R/lib/SparkR"
    if (file(sparkR).exists()) {
      copy {
        from sparkR
        into "${snappyProductDir}/R/lib"
      }
    }
  }
}

def copyTestsCommonResources(def bdir) {
  def outdir = "${bdir}/resources/test"
  file(outdir).mkdirs()

  copy {
    from "${rootDir}/tests-common/src/main/resources"
    into outdir
  }
}

def runScript(def execName, def param) {
def stdout = new ByteArrayOutputStream()
    exec{
      executable "${execName}"
      args  (param)
      standardOutput = stdout;
    }
  return "$stdout"
}

def runSQLScript(def fileName) {
  println("Executing ${fileName}")
  def queryoutput = runScript("${snappyProductDir}/bin/snappy-shell",
          ["run", "-file=${snappyProductDir}/quickstart/scripts/${fileName}"]);
  println "${queryoutput}"
  if (queryoutput.contains("ERROR") || queryoutput.contains("Failed")) {
    throw new GradleException("Failed to run ${fileName}")
  }
}

task runQuickstart {
  mustRunAfter product
  def exampleArchiveName = "quickstart-${version}.jar"
  doLast {
    try {
      def startoutput = runScript("${snappyProductDir}/sbin/snappy-start-all.sh", []);
      println "${startoutput}"
      if (!startoutput.contains("Distributed system now has 4 members")) {
        throw new GradleException("Failed to start Snappy cluster")
      }
      runSQLScript("create_and_load_column_table.sql")

      runSQLScript("create_and_load_row_table.sql")

      runSQLScript("status_queries.sql")

      runSQLScript("olap_queries.sql")

      runSQLScript("oltp_queries.sql")

      runSQLScript("olap_queries.sql")

      def startSparkResult = runScript("${snappyProductDir}/sbin/start-all.sh", [])
      println "${startSparkResult}"

      def airlineappresult = runScript("${snappyProductDir}/bin/spark-submit",
              ["--class", "io.snappydata.examples.AirlineDataSparkApp", "--master", "spark://localhost:7077",
               "--conf", "snappydata.store.locators=localhost:10334", "--conf", "spark.ui.port=4041",
               "${snappyProductDir}/lib/${exampleArchiveName}"]);

      println "${airlineappresult}"
      if (airlineappresult.toLowerCase().contains("exception")) {
        throw new GradleException("Failed to submit AirlineDataSparkApp")
      }

      def submitjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh",
              ["submit", "--lead", "localhost:8090", "--app-name", "myjob",
               "--class", "io.snappydata.examples.AirlineDataJob", "--app-jar",
               "${snappyProductDir}/lib/${exampleArchiveName}"]);

      println "${submitjobairline}"

      def json = new JsonSlurper().parseText("${submitjobairline}".substring(2))
      def jobid = ""
      json.each {
        k, v ->
          if (k == "result") {
            if (v instanceof groovy.json.internal.LazyMap) {
              jobid = v.get("jobId")
            }
          }
      }

      def status = "RUNNING"
      while (status == "RUNNING") {
        Thread.sleep(3000)
        def statusjobairline = runScript("${snappyProductDir}/bin/snappy-job.sh",
                ["status", "--lead", "localhost:8090", "--job-id", "${jobid}"]);
        println "${statusjobairline}"

        def statusjson = new JsonSlurper().parseText("${statusjobairline}")
        statusjson.each {
          k, v ->
            if (k == "status") {
              println("Current status of job: " + v)
              status = v
            }
        }
      }
      if (status == "ERROR") {
        throw new GradleException("Failed to submit queries")
      }


    } finally {
      println runScript("${snappyProductDir}/sbin/snappy-stop-all.sh", []);
      println runScript("${snappyProductDir}/sbin/stop-all.sh", []);
    }
  }
}

task copyResourcesAll << {
  copyTestsCommonResources(project(":snappy-core_${scalaBinaryVersion}").buildDir)
  copyTestsCommonResources(project(":snappy-tools_${scalaBinaryVersion}").buildDir)
  copyTestsCommonResources(project(":snappy-aqp").buildDir)
  copyTestsCommonResources(project(':snappy-dunits').buildDir)
}

task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  mustRunAfter cleanAll
}
task checkAll {
  dependsOn  ":snappy-core_${scalaBinaryVersion}:check", ":snappy-tools_${scalaBinaryVersion}:check", ':snappy-aqp:check', ':snappy-dunits:test'
  if (project.hasProperty('spark')) {
    dependsOn project(':snappy-spark').getTasksByName('check', true).collect { it.path }
  }
  mustRunAfter buildAll
  mustRunAfter product
}
task precheckin {
  dependsOn cleanAll, buildAll, product, checkAll, runQuickstart
}
